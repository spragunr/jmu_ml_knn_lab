{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors\n",
    "\n",
    "Learning objectives:\n",
    "* Gain an appreciation for the strengths and weaknesses of KNN classifiers.\n",
    "* Explore the computiational requirements of neighborhood searches for KNN.\n",
    "\n",
    "## Exercise 1 - Irisis\n",
    "\n",
    "The code in the cell below loads a modified version of the famous Iris data set. The point of this data set is to predict the species of a flower by examining several measurements taken from the petals.  This is considered an easy classificaion problem because the three different classes are well separated in feature space. \n",
    "\n",
    "I've modified this data set in a way that makes classification more difficult.\n",
    "\n",
    "Update the cell below to use cross-validation to tune both the decision tree and KNN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datasource\n",
    "from sklearn import tree\n",
    "from sklearn import neighbors\n",
    "from sklearn import model_selection \n",
    "\n",
    "X, y = datasource.get_iris_data()\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y,\n",
    "                                                                    train_size=0.8,\n",
    "                                                                    test_size=0.2,\n",
    "                                                                    random_state=20,\n",
    "                                                                    stratify=y)\n",
    "\n",
    "tree = tree.DecisionTreeClassifier(max_leaf_nodes = 2)\n",
    "# Should do some hyperparameter tuning here...\n",
    "tree.fit(X_train, y_train)\n",
    "print(\"Decision tree accuracy: {:.4f}\".format(tree.score(X_test, y_test)))\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
    "# Should do some hyperparameter tuning here...\n",
    "knn.fit(X_train, y_train)\n",
    "print(\"KNN accuracy:           {:.4f}\".format(knn.score(X_test, y_test)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: \n",
    "* What is your error rate on the test set for your best hyperparameter settings for the two classifiers? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer: \n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - Data Pre-processing\n",
    "\n",
    "You probably found that one of the classifiers above was able to acheive a significantly lower error rate than the other.  Your goal now is to solve the following two problems:\n",
    "\n",
    "1. Determine what it is about the provided data that makes this problem so much harder for one classifier than the other.\n",
    "2. Perform pre-processing on the data to make classification easier for the struggling classifier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "* Describe a scenario where KNN would be a better choice than a decision tree.\n",
    "* Describe a scenario where a decision tree sould be a better choice than KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers\n",
    "* \n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 - KNN Efficiency\n",
    "\n",
    "\n",
    "The cell below will time KNN lookups using brute-force searches then plot the results as a function of the size of the dataset. BEFORE RUNNING THE CELL, make a prediction about the trend that you expect to see.  Discuss your prediction with your peers, then run the cell and check your answer.  Try repeating this experiment using `kd_tree` instead of `brute` as the lookup algorithm. \n",
    "\n",
    "Now try re-running this experiment with a 100-dimensional data set instead of a four-dimensional data set.  How does this change the results?  (You may need to reduce the maximum size in order to get the results in within a reasonable amount of time.) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import time\n",
    "\n",
    "times = []\n",
    "\n",
    "max_size = 1000000\n",
    "increment = max_size // 10\n",
    "start = max_size // 10\n",
    "\n",
    "sizes = range(start, max_size, increment)\n",
    "print(\"Timing\", end='')\n",
    "for num in sizes:\n",
    "    print(\".\", end='')\n",
    "    X, y = datasets.make_classification(n_samples=num, n_features=4, n_classes=2)\n",
    "    # Algorithm can be either 'brute' or 'kd_tree'\n",
    "    knn = neighbors.KNeighborsClassifier(n_neighbors=5, algorithm='brute')\n",
    "    knn.fit(X,y)\n",
    "    trials = 100\n",
    "    start = time.time()\n",
    "    for i in range(trials):\n",
    "        y = knn.predict([X[np.random.randint(X.shape[0]), :]])\n",
    "    times.append((time.time() - start)/trials)\n",
    "    \n",
    "plt.plot(sizes, times)\n",
    "plt.xlabel('data size')\n",
    "plt.ylabel('time per lookup (s)')\n",
    "plt.show()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
